# Daily Paper Reading

#### 2022/12/19

- [x] Reproducible scaling laws for contrastive language-image learning: \[[Paper](https://arxiv.org/pdf/2212.07143.pdf)]

#### 2022/12/25

- [x] Scaling Language-Image Pre-training via Masking: \[[Paper](https://arxiv.org/pdf/2212.00794.pdf)]
- [x] CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet: \[[Paper](https://arxiv.org/pdf/2212.06138)]

#### 2022/12/26

- [x] Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models: \[[Paper](https://arxiv.org/pdf/2212.03860)]
- [x] Multimodal Masked Autoencoders Learn Transferable Representations: \[[Paper](https://arxiv.org/abs/2205.14204)]

#### 2022/12/27

- [x] RegMixup: Mixup as a Regularizer Can Surprisingly Improve Accuracy & Out-of-Distribution Robustness: \[[Paper](https://arxiv.org/abs/2206.14502)]

#### 2022/12/28

- [x] How to Fine-Tune Vision Models with SGD: \[[Paper](https://arxiv.org/abs/2211.09359)]
- [x] Attentive Mask CLIP: \[[Paper](https://arxiv.org/abs/2212.08653)]
- [x] MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining: \[[Paper](https://arxiv.org/abs/2208.12262)]

#### 2022/12/29

- [ ] Cramming: Training a Language Model on a Single GPU in One Day: \[[Paper](https://arxiv.org/abs/2212.14034)]
- [ ] How Well Do Self-Supervised Models Transfer?: \[[Paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Ericsson_How_Well_Do_Self-Supervised_Models_Transfer_CVPR_2021_paper.pdf)]

#### 2022/12/30

- [ ] What Knowledge Gets Distilled in Knowledge Distillation?: \[[Paper](https://arxiv.org/abs/2205.16004)]
