# Daily Paper Reading

#### 2022/12/19

- [x] Reproducible scaling laws for contrastive language-image learning: \[[Paper](https://arxiv.org/pdf/2212.07143.pdf)]

#### 2022/12/25

- [x] Scaling Language-Image Pre-training via Masking: \[[Paper](https://arxiv.org/pdf/2212.00794.pdf)]
- [x] CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet: \[[Paper](https://arxiv.org/pdf/2212.06138)]

#### 2022/12/26

- [x] Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models: \[[Paper](https://arxiv.org/pdf/2212.03860)]
- [x] Multimodal Masked Autoencoders Learn Transferable Representations: \[[Paper](https://arxiv.org/abs/2205.14204)]

#### 2022/12/27

- [x] RegMixup: Mixup as a Regularizer Can Surprisingly Improve Accuracy & Out-of-Distribution Robustness: \[[Paper](RegMixup: Mixup as a Regularizer Can Surprisingly Improve Accuracy & Out-of-Distribution Robustness)]

#### 2022/12/28

- [ ] How to Fine-Tune Vision Models with SGD: \[[Paper](https://arxiv.org/abs/2211.09359)]
- [ ] Attentive Mask CLIP: \[[Paper](https://arxiv.org/abs/2212.08653)]
